\documentclass{article}


%% Bring the margins down to 1 inch, like the old ``fullpage'' package.
\usepackage[margin=1.0in]{geometry}

\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{hyperref}

%% Gives the equivalent of one-and-a-half line spacing.
\linespread{1.3}

\begin{document}

\section{Data description and organization}

Our goal is to estimate the accumulated degree days post mortem based
on the types and prevalences of bacterial and eukaryotic organisms
present on pig cadavers.  We made use of the order-level and
family-level taxa which were obtained from 6 pig cadavers over the
course of the study, for which data were collected at intervals during
a period of 61 days.  We decided to focus on the family and order
levels because our early results indicated that analyses at the
phylum-level did not explain nearly as much of the variability as did
models built on family-level and order-level taxa.  At lower, finer
levels (such as genus), we anticipate that an unreasonably large
number of counts would remain unclassified.

Samples were obtained from 6 pig cadavers on 16 different days over
the course of the 61-day period.  Samples were taken more frequently
during the first 15 days (approx.~two weeks) at the beginning of the
study, which is when we would expect rapid changes to occur.  To
account for effect of temperature, we utilized accumulated degree
days, rather than the number of days post mortem.  Table
\ref{tbl:degdays_vs_days} shows all the days on which samples were
taken, along with the corresponding accumulated degree days (ADD).
\begin{table}[hb]
  \centering
  \caption{\label{tbl:degdays_vs_days}Days and accumulated degree days
    (ADD) post mortem}
  \begin{tabular}{r|rrrrrrrrrrrrrrrr}
  Days & 0 & 1 & 2 & 3 & 5 & 7 & 9 & 11 & 13 & 15 & 26 & 33 & 40 & 47 & 54 & 61\\ \hline
  ADD & 0 & 27 & 57 & 87 & 149 & 209 & 267 & 326 & 390 & 448 & 734 & 930 & 1130 & 1326 & 1516 & 1703
  \end{tabular}
\end{table}

Figure \ref{fig:degdays_vs_days} shows that the relationship between
days and ADD is almost linear throughout the study time period.  The
estimated correlation coefficient between calendar days post mortem
and ADD post mortem exceeds 0.99.  This is true when accounting for
the entire time period, or when examining only the first 15 days.
\begin{figure}[hb]
  \centering
  \includegraphics[height=3.5in]{degdays_vs_days}
  \caption{Accumulated degree days vs.~days post mortem}
  \label{fig:degdays_vs_days}
\end{figure}

Whether working with bacteria or eukaryote data, we did not directly
utilize raw counts in our statistical models.  Exploratory analyses
showed a great deal of variability in the raw counts between
individual cadavers and between days.  Raw counts would also be likely
to vary widely between experiments, which would make our model less
applicable to future experiments.  Instead, we used the proportion of
the counts attributable to each taxon.  First, we excluded all counts
of unclassified taxa from the dataset.  Then, for each taxon, we
calculated the fraction of the counts corresponding to that taxon for
that particular sample (i.e. for that day and for that cadaver).  To
be included in our models, a taxon had to make up at least 1\% of the
daily taxa counts for at least 2 different cadavers, whether on the
same day or on different days.  All taxa which did not meet this
criterion were grouped together as ``rare'' taxa.  This ``rare''
category was not included in our predictive models.  All fractions
attributable to non-rare taxa for individual cadaver and each day were
utilized as potential explanatory variables, while the ADD post mortem
represented the response variable.

While the requirements related to the determination of rare taxa may
seem strange, the idea is to prevent a rare taxon found on just one
cadaver from adversely affecting the model.  In my earlier models
using bacterial family-level taxa, it happened that a particular
taxon, Listeriaceae, which made up more than 1\% of the taxa on only
one cadaver for only one day was nevertheless selected as an important
predictor in the final model.  My concern is that Listeriaceae may not
be present on future observed cadavers, or, because it is at such low
levels, it may not always be easily measured.  To obtain a more usable
model, I wanted to avoid including taxa which appeared very
infrequently in our samples, or were introduced solely through one
unusual cadaver.  I believe the question of which taxa are ``common
enough'' to be utilized in a predictive model is an important one, and
these criteria may need to be examined in more detail.


\section{Methods}

\subsection{Introduction to ``random forests''}

Inspired by Metcalf et al.~(2016), I made use of random forest models.
The random forest methodology is non-parametric, so it does not depend
on distributional assumptions nor on identifying a mathematical
function to match an apparent trend.  It is commonly employed in
machine learning applications, since it can accommodate a large number
of potential explanatory variables and can be used even when there are
dependencies/associations among these explanatory variables.

A good description of the random forest methodology is provided by
James et al. (2013, Chp.~8), the text of which is freely available
from the first author's website at 
\href{https://www-bcf.usc.edu/~gareth/ISL/}{https://www-bcf.usc.edu/~gareth/ISL/}.  The
approach uses tree-based models to split the observations into groups,
step by step at each branch of the tree.  At each split, we divide the
observations in into two groups in which the variability among the
response values in each group is minimized.  The ``forest'' part of
the ``random forest'' name comes from the fact that we fit multiple
trees (i.e., a ``forest'') using bootstrap samples of the dataset of
interest, rather than just one tree.  The final model can be viewed as
an average of the ``forest'' of models.  To reduce the correlation
among the trees in the ``forest'', we consider only a random subset of
the potential explanatory variables at each split.  So, for the
``random forest'' approach, there are two random aspects: the
bootstrap samples are generated randomly and the a random subset of
explanatory variables is considered at each split.  One consequence is
that another analyst's result will typically not be exactly
reproducible.

\subsection{Choosing the number of variables to consider at each ``split''}

It is not immediately evident how many explanatory variables should be
considered at each split of the tree, so this number is a parameter
that needs to be estimated.  We use cross-validation to help us choose
the best number for our random forest model, which I called
\texttt{numVarSplit} in my R code.  Another parameter often considered
in such models is the number of bootstrapped trees sufficient for the
error to converge to a minimum.  In the course of this work, as well
as in the examples provided by James et al.\ (2013), 3000 bootstrapped
trees appeared to well exceed the number required, so I used 3000
trees for each model.

To determine the best setting for the \texttt{numVarSplit} parameter,
I used cross-validation, which involved fitting the random forest
model multiple times (``runs'').  For each run, I randomly selected
20\% of the available observations to hold back as a ``validation''
data set.  The remaining 80\% of the observations then made up the
``training'' dataset.  For each training/validation partition, I fit a
series of random forest models on the training data, using a variety
of possible values for \texttt{numVarSplit}, and making predictions
for the values in the associated validation set.  Then, I calculate
the errors, which are given by the difference between the actual ADD
from the validation dataset and the number of degree days estimated by
the model on the basis of the training data.  I then select the value
for \texttt{numVarSplit} which produced the smallest mean square
errors and the explained the largest fractions of variability.

Cross-validation procedures are critical tools for fitting random
forest models and evaluating their performance.  If we used all our
data to train the model, with no cross-validation or testing, we would
be likely to ``overfit'' or ``overtune'' our model, so that it fits
the training dataset well, but provided biased results when utilized
in prediction mode for future datasets.  However, unlike many machine
learning approaches, we have a very limited number of observations to
work with.  We have only 6 cadavers, and we only have observations at
certain time steps.  I was concerned about finding a reasonable
balance of training vs.~validation percentages.

Since our earlier conversations about our original modeling attempts,
for each cross-validation run, I've reduced the training set to 80\%
of the data, with 20\% for validation.  (Previously, the split was
90\% and 10\%.)  I used 1000 cross-validation runs, which means each
observation is randomly assigned to the training or validation group
1000 times.  When using the bacterial data for the whole study period
(all time steps), this means training on a set of 74 observations, and
testing on a size of 18.  When using just the first 15 days of
bacterial data, this means training on a set of 46 observations, and
testing on a size of 11.  The change from 90\% to 80\% made very
little difference in the choice of models, which relieved some of my
concern.  Also, since our earlier meetings, I figured out how to
parallelize some of the computational work, which allowed me to do
some additional cross-validation runs and try additional models,
compared to what I had done previously.


\subsection{Understanding variability in the final fit}

Using the cross-validation runs mentioned above, I was able to choose
an appropriate value for the \texttt{numVarSplit}, the number of
variables to consider at each tree ``split''.  Different values of
\texttt{numVarSplit} were determined for the various models: bacteria
vs.~ eukaryote, family-level vs.~order-level taxa, all time steps
vs.~first 15 days.  Once this parameter was determined, I generated a
``final'' random forest model using the entire dataset for the case
under consideration.  I calculated the traditional model fit
statistics, the root mean square error (RMSE) and a measure sometimes
called ``pseudo R squared''.  The pseudo R-squared is an approximation
of the fraction of variation in the response variable which can be
accounted for by the model, and is given by 1 - (mean square error /
Variance of ADD).

However, recall that the model fitting procedure depends on the random
generation of bootstrapped samples from the training dataset and the
random selection of which variables to consider at each split of the
decision tree.  Therefore, the final model, using the same complete
dataset and parameters, will differ slightly from one execution to the
next or one practitioner to another.  To get a sense of the
variability associated with this necessary randomness, I fit the final
model over and over again, and looked at how the RMSE and the pseudo
R-squared vary.  Table \ref{tbl:final_model_stats} shows
these statistics for the various models.

\begin{table}
  \centering
\caption{\label{tbl:final_model_stats}Model fit statistics using
  complete datasets}
\begin{tabular}{lllrr}
Period considered & Source data & Level & RMSE & Pseudo R-squared\\ \hline \hline
Entire study & Bacteria  & Family-level & 208 ($\pm$ 3) & 85.9\% ($\pm$ 0.4\%)\\
Entire study & Eukaryote & Family-level & 178 ($\pm$ 2) & 89.6\% ($\pm$ 0.3\%)\\
Entire study & Combined  & Family-level & 174 ($\pm$ 3) & 90.2\% ($\pm$ 0.3\%)\\ \hline
Entire study & Bacteria  & Order-level & 232 ($\pm$ 2) & 82.5\% ($\pm$ 0.4\%)\\
Entire study & Eukaryote & Order-level & 194 ($\pm$ 2) & 87.6\% ($\pm$ 0.3\%)\\
Entire study & Combined  & Order-level & 197 ($\pm$ 3) & 87.4\% ($\pm$ 0.3\%)\\ \hline
First 15 days & Bacteria  & Family-level &  64 ($\pm$ 1) & 82.3\% ($\pm$ 0.5\%)\\
First 15 days & Eukaryote & Family-level &  59 ($\pm$ 1) & 84.5\% ($\pm$ 0.5\%)\\ \hline
First 15 days & Bacteria  & Order-level  &  54 ($\pm$ 1) & 87.4\% ($\pm$ 0.3\%)\\
First 15 days & Eukaryote & Order-level  &  64 ($\pm$ 1) & 81.6\% ($\pm$ 0.5\%)
\end{tabular}
\end{table}


\subsection{Thinking about the likely error associated with using the models in ``real-life''}

One of my concerns is that while the techniques described above are
good ways to assess a model's fit and performance, these approaches do
not reflect the particular issues involved in how we would
realistically use this model.  In practice, we would be using this
model to predict the ADD post mortem based on the counts of
family-level or order-level taxa from a never-before-observed cadaver.
The actual post mortem interval in such a case would be unlikely to
correspond with a number of accumulated degree days which we have
previously observed in our experiments.  For instance, we might be
dealing with a cadaver for which the post mortem interval is actually
100 or 500 ADD, while our dataset contains observations at 87 ADD and
448 ADD.

To get a better idea about how the model might be expected to perform
in such a situation, we fit the model for using a series of carefully
chosen cross-validation sets.  For each set, we leave out all the
observations associated with one particular cadaver (for all days in
the study period) and all the observations associated with one
particular day (for all cadavers).  Then, we fit the model using the
remaining days' and cadavers' observations, calculating the prediction
error for the left-out cadaver on the left-out day.  This procedure is
intended to mimic the real-life scenario in which the scientist is
interested in making a prediction of ADD for a cadaver which has not
been previously observed and for a number of accumulated degree days
which has not been observed.  For example, when dealing with the
eukaryotic taxa observed across the entire study period, we would have
96 cross-validation sets for consideration(16 days $\times$ 6
cadavers) .  In each case, the training set would consist of 75
observations (15 days $\times$ 5 cadavers) and the validation set would
contain the one observation corresponding to the omitted cadaver on
the omitted day.

For comparison purposes, I also performed traditional cross-validation
runs, in which I randomly selected training datasets of the same size
as that used in my ``real-life'' validation strategy above.  While
this approach is more conventional, it does not reflect our greater
realistic uncertainty, because it is unlikely that a random training
set would be chosen which completely omits one day and one cadaver.
However, I think these statistics are useful for comparison
purposes.  

Table \ref{tbl:valid_model_stats} shows the average RMSE for the
random cross-validation sets and the real-life (leaving out all
observations associated with a specific cadaver and day).  For the
random cross-validation procedure, I did 1000 runs, with the training
set selected randomly each time and the size of the training set
matching the size of the training set for the ``real-life''
cross-validation runs.  Because there are so many more runs, and
because the make-up of the training sets can vary widely, there is
much more variability associated with RMSE from run to run.  The
``+/-'' portion in parentheses gives a margin of error, which is
approximated by 1.96 $\times$ the standard deviation of the RMSEs from
the 1000 runs.  For the cross-validation runs reflecting the
``real-life'' scenarios, there are a limited number of
cross-validation sets possible.  Although I tried fitting each of
these cross-validation sets numerous times, the variation was
negligible (on the order of $\pm 1$ or $\pm 2$) compared to that from
the random cross-validation runs.

\begin{table}
\centering
\caption{\label{tbl:valid_model_stats}Cross-validation prediction
  performance statistics, using random and ``real-life''
  cross-validation strategies}
\begin{tabular}{lllrr}
Period considered & Source data & Level & RMSE (random) & RMSE (``real-life'')\\ \hline \hline
Entire study & Bacteria  & Family-level & 219 ($\pm$ 79)  & 276\\
Entire study & Eukaryote & Family-level & 187 ($\pm$ 78)  & 244\\
Entire study & Combined  & Family-level & 183 ($\pm$ 72)  & 242\\ \hline
Entire study & Bacteria  & Order-level  & 244 ($\pm$ 102) & 324\\
Entire study & Eukaryote & Order-level  & 202 ($\pm$ 88)  & 252\\
Entire study & Combined  & Order-level  & 204 ($\pm$ 84)  & 269\\ \hline
First 15 days & Bacteria  & Family-level & 67 ($\pm$ 28) & 84\\
First 15 days & Eukaryote & Family-level & 64 ($\pm$ 26) & 93\\ \hline
First 15 days & Bacteria  & Order-level  & 56 ($\pm$ 29) & 83\\
First 15 days & Eukaryote & Order-level  & 69 ($\pm$ 29) & 92
\end{tabular}
\end{table}




\section{Reviewing models for the entire study period (days 0-61)}


\subsection{Using Shanes's bacteria data}

For the bacterial taxa (originally from Shane), we had to account for
missing data.  Taxa counts were unavailable for subject A1 on days 7
and 9, as well as for subject A4 on day 7.  In addition, the data
collected from subject A3 on day 40 (degree day 1130) is defective.
The total number of taxa (including unclassified taxa) recorded for
this cadaver and day was 54.  This is several orders of magnitude less
than the counts observed on the other cadavers and during other days,
which ranged from 18050 to 93665 (both orders and families).  For this
reason, the taxa from subject A3 on day 40 were omitted.  (Note that
the omission of the A3, day 40 data does not affect the analyses which
only consider the first 15 days of the study.)  This leaves us with a
total of 92 observations (16 days $\times$ 6 cadavers, minus the 4
missing cadaver-day combinations).  Across all days and cadavers,
about 29.3\% of the taxa were ``unclassified''.  There were 39
family-level taxa considered as potential predictors, when doing
analyses for the whole time period.

With reference to Tables \ref{tbl:final_model_stats} and
\ref{tbl:valid_model_stats}, the family-level bacterial taxa seem to
do a better job at prediction than the order-level bacterial taxa.
The RMSE values are lower for the family-level taxa, whether looking
at the model fit statistics or the prediction performance statistics.
So, for the remainder of this sub-section, I'll focus on the
family-level taxa.

Figure \ref{fig:infl_bac_family_taxa} shows measures of model
importance for the 6 most influential taxa.  (I have chosen 6
simply because it's easy to show a 2 by 3 panel of plots.  If we
display more, it tends to be too much information to easily digest.)
The measure of importance is the mean percentage increase in the
mean-square error of model predictions, when we consider models in
which the variable in question is left out of the model.  It should be
noted that this measure is one of the two commonly used methods for
measuring the importance of the individual taxa; the other measure,
called ``node purity'', is harder to explain.  These measures do
sometimes differ on the ranking of taxa influence, as there is more
than one way to judge the relevance of a particular predictor.
However, in the course of doing these analyses, I've noticed that both
measures generally feature the same important taxa, though their
relative rankings may be different.
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{../revise_algorithm/only_families/all_time_steps/hit_1perc_twice/orig_units_all_data_families_PercIncMSE_barchart}
  \caption{Most influential bacterial family-level taxa (all time steps)}
  \label{fig:infl_bac_family_taxa}
\end{figure}

Figure \ref{fig:infl_bac_family_scatter} provides scatterplots
corresponding to these six taxa.  Each point in the plots below
corresponds to the measured fraction of classified bacteria belonging
to the designated taxa on each of the degree days.  Note that the
scales of the vertical axes differ among the taxa.  From this figure,
we can easily see how much variation there is from not just from
day-to-day, but from one cadaver to another.
\begin{figure}
  \centering
  \includegraphics[width=6.5in]{../revise_algorithm/only_families/all_time_steps/hit_1perc_twice/infl_bac_family_all_data_scatter}
  \caption{Scatterplots for influential bacterial family-level taxa (entire study period)}
  \label{fig:infl_bac_family_scatter}
\end{figure}

Figure \ref{fig:leave_one_out_resids_bac_family_taxa} shows the
residuals for all combinations of degree day and subject.  The pattern
seen here is typical of the pattern in all the residual plots I've
examined.  Given uncertainty, most statistical models will tend to err
on the side of an estimation closer to the mean response level.  At
the two temporal extremes (near the start and end of the study
period), this behavior is particularly obvious.  Early in the period,
the residuals are negative because the model prediction is higher than
the actual ADD.  The opposite is true near the end of the study
period.  In the middle of the study period, the residuals are more
equally distributed around zero.
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{../revise_algorithm/only_families/all_time_steps/hit_1perc_twice/leave_out_one_subj_and_one_day_residuals}
  \caption{Residuals for model runs leaving out one subject and one day (bacterial family-level taxa)}
  \label{fig:leave_one_out_resids_bac_family_taxa}
\end{figure}


\subsection{Using Luisa's eukaryote data}

In the eukaryote data, we don't have missing data for any day-subject
combinations.  Therefore, we have a total of 96 observations (16 days
$\times$ 6 cadavers), which is 4 more observations than were available
with the bacterial data.  The choice of whether to use order-level or
family-level taxa is not as clear for the eukaryote data as it was for
the bacterial data.  For brevity, I will focus on the family-level
taxa, since the associated RMSEs are lower.  Across all days and
cadavers, about 32.76\% of the family-level taxa were unclassified.
There were 39 family-level taxa considered as potential predictors
when doing analyses for the entire time period.

We used the same measure of taxa importance as that used for the
bacteria family-model in the previous section, though we omit the
graphic for brevity.  These taxa are featured in Figure
\ref{fig:infl_euk_family_all_data_scatter}, which shows how the
fraction of counts attributed to them change over the time period and
across individual cadavers.  As before, note that the scales of the
vertical axes are different among the six scatterplots.
\begin{figure}
  \centering
  \includegraphics[width=6.5in]{../eukaryote_data/only_families/all_time_steps/hit_1perc_twice/infl_euk_family_all_data_scatter}
  \caption{Scatterplots for influential eukaryotic family-level taxa (entire study period)}
  \label{fig:infl_euk_family_all_data_scatter}
\end{figure}

The final model using the eukaryote data appears to be a better in
terms of both fit and prediction than that using the bacteria data.
This is indicated by the model performance statistics featured in
Tables \ref{tbl:final_model_stats} and \ref{tbl:valid_model_stats}.
Also, the residual plot associated with the ``real-life''
cross-validation procedure in Figure
\ref{fig:leave_one_out_resids_euk_family_taxa} shows that the
residuals for the model utilizing the eukaryote data have a smaller
range.  They also show somewhat less over- and under-prediction at the
temporal extremes.
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{../eukaryote_data/only_families/all_time_steps/hit_1perc_twice/leave_out_one_subj_and_one_day_residuals}
  \caption{Residuals for model runs leaving out one subject and one day (eukaryotic family-level taxa)}
  \label{fig:leave_one_out_resids_euk_family_taxa}
\end{figure}

In addition, I did try combining both bacterial and eukaryotic taxa.
This procedure was more complicated to implement, because there are
different strategies for combining the datasets.  I tried one method
that equally weighted bacterial and eukaryotic information sources,
which was a bit tricky to implement and to interpret.  I also tried
combining them without the constraint that the information from both
sources be equally weighted, which is the method I used to provide the
statistics found in Tables \ref{tbl:final_model_stats} and
\ref{tbl:valid_model_stats}.  I found that regardless of which of
these 2 methods I used, combining the two datasets did not give an
improvement substantial enough, in my opinion, to justify the
``costs'' associated with preparing and interpreting the analysis.


\section{Models using only the first 15 days of the study period}

\subsection{Using Shanes's bacteria data}

For the analyses considering the first 15 days (approximately two
weeks), we have 57 observations.  These represent observations on 6
cadavers on 10 observation days, minus the 3 missing cadaver-day
combinations occurring during this period.  This includes the missing
data for subject A1 on days 7 and 9 and the missing data for subject
A4 on day 7 of the study.

Somewhat surprisingly, for the bacteria data, the order-level taxa
seem to provide a slightly better fit, though this advantage is quite
slim when looking at the ``real-life'' RMSE in Table
\ref{tbl:valid_model_stats}.  For the order-level taxa in just the
first 15 days, 14 taxa serve as possible explanatory variables.  The
scatterplots for the six most influential taxa, are shown in Figure
\ref{fig:infl_bac_first_15_days_family_scatter} (once again with
varying vertical scales among panels).  For comparison with the
earlier models for the entire time period, the residual scatterplot is
included in Figure \ref{fig:leave_one_out_resids_bac_order_taxa_first_15}.
\begin{figure}
  \centering
  \includegraphics[width=6.5in]{../revise_algorithm/only_orders/first_two_weeks/hit_1perc_twice/infl_bac_order_first_two_weeks_scatter}
  \caption{Scatterplots for influential bacterial order-level taxa (first 15 days)}
  \label{fig:infl_bac_first_15_days_family_scatter}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{../revise_algorithm/only_orders/first_two_weeks/hit_1perc_twice/leave_out_one_subj_and_one_day_residuals}
  \caption{Residuals for model runs leaving out one subject and one day (bacterial order-level taxa, first 15 days)}
  \label{fig:leave_one_out_resids_bac_order_taxa_first_15}
\end{figure}



\subsection{Using Luisa's eukaryote data}

Again, for the eukaryote taxa, we don't have missing observations, so
we have a total of 60 observations.  The performance of the models
based on family-level and order-level taxa are comparable.  While one
could argue that there's a slight edge in going with the family-level
taxa, the models really don't appear to be substantially different.  I
will present the model for the family-level taxa, but I can provide
similar plots for the order-level model.  There are 37 family-level
taxa which can be used as explanatory variables in the model; the six
most influential in our model are shown in Figure
\ref{fig:infl_euk_family_scatter_first_15}.  Also for comparison, the
residual plot for the ``real-life'' validation case is shown in Figure
\ref{fig:leave_one_out_resids_euk_family_taxa_first_15}.
\begin{figure}
  \centering
  \includegraphics[width=6.5in]{../eukaryote_data/only_families/first_two_weeks/hit_1perc_twice/infl_euk_family_first_two_weeks_scatter}
  \caption{Scatterplots for influential eukaryotic family-level taxa (first 15 days)}
  \label{fig:infl_euk_family_scatter_first_15}
\end{figure}
\begin{figure}
  \centering
  \includegraphics[width=3.5in]{../eukaryote_data/only_families/first_two_weeks/hit_1perc_twice/leave_out_one_subj_and_one_day_residuals}
  \caption{Residuals for model runs leaving out one subject and one day (eukaryotic family-level taxa, first 15 days)}
  \label{fig:leave_one_out_resids_euk_family_taxa_first_15}
\end{figure}

Because the analyses for the combined eukaryote and bacterial data for
the entire study period did not seem to offer a large improvement over
either by itself, I did not make a detailed analysis for the combined
data limited to the first 15 days of observations.


\section{Discussion}

I think there are several topics here worthy of further discussion.
The first question is one I mentioned in the data description section.
How should we decide which taxa are prevalent enough to be included as
possible explanatory variables in a random forest model?  Several of
the taxa which were indicated to be influential in our models make up
small percentages of the taxa count.  For example, three of the six
influential taxa using the bacteria taxa for the entire study period
are present at levels no higher than 5\%, as shown in Figure
\ref{fig:infl_bac_family_scatter}.  One of these, Bifidobacteriaceae,
is present at levels no higher than 2\%.  A few months ago, I
experimented with a 10\% or so cutoff, but based on some early-on
model fit statistics, I became concerned that using a cutoff so
stringent was having an adverse impact.  I think it would be wise to
consider how to draw this cutoff appropriately.

It would also be helpful to test our models on some other appropriate
dataset(s), if possible.  This would give us a better sense of the
shortcomings of our model.  One thing that I'm particularly concerned
about is whether a new dataset would necessarily contain observations
on the same taxa that figured prominently in the random forest models
that we have fit during this process.  It would also allow us to test
the real-life situation in which we try to make predictions about a
cadaver and a number of ADDs which our models have never seen before.
This is likely to be the best way to test our method's practical
utility.


\section{References}

\noindent James, G., Witten, D., Hastie, T., Tibshirani, R. (2013) \textit{An
  Introduction to Statistical Learning with Applications in R}. New
York: Springer

\noindent Metcalf, J. L., et al. (2016).  Microbial community assembly and
metabolic function during mammalian corpse decomposition.
\textit{Science}, Vol.~351, 158-162.

\noindent R Core Team (2018). R: A language and environment for
statistical computing. R Foundation for Statistical Computing, Vienna,
Austria. URL: https://www.R-project.org/.

\end{document}
